{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebc2e716-8e76-462a-a34b-76636ee27efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/root/miniconda3/envs/shaos2/lib/python3.10/site-packages/torch_npu/__init__.py:234: UserWarning: On the interactive interface, the value of TASK_QUEUE_ENABLE is set to 0 by default.                      Do not set it to 1 to prevent some unknown errors\n",
      "  warnings.warn(\"On the interactive interface, the value of TASK_QUEUE_ENABLE is set to 0 by default. \\\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb152cd75df4ba0b1ff01b8ff1bb3d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] init success\n",
      "Generated Output: The capital of France is Paris.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "from transformers import AutoTokenizer\n",
    "from threading import Lock\n",
    "from utils.session import Session\n",
    "from config import InferenceConfig\n",
    "import torch\n",
    "\n",
    "class Inference:\n",
    "    def __init__(self, config: InferenceConfig) -> None:\n",
    "        # Initialize the necessary variables\n",
    "        self.max_input_length = config.max_input_length\n",
    "        self.max_output_length = config.max_output_length\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            config.tokenizer_dir, trust_remote_code=True\n",
    "        )\n",
    "        self.session = Session.fromConfig(config)\n",
    "        self.session_type = config.session_type\n",
    "        self.kv_cache_length = config.kv_cache_length\n",
    "        self.state: dict = {\"code\": 200, \"isEnd\": False, \"message\": \"\"}\n",
    "        self.reset()\n",
    "        self.lock = Lock()\n",
    "        self.first = True\n",
    "        print(\"[INFO] init success\")\n",
    "\n",
    "    def generate_cache(self, prompt: str):\n",
    "        if len(prompt) == 0:\n",
    "            return\n",
    "        self.first = False\n",
    "        input_ids = np.asarray(self.tokenizer.encode(prompt), dtype=np.int64).reshape(1, -1)\n",
    "        logits = self.session.run(input_ids)[0]\n",
    "        next_token = self.sample_logits(logits[0][-1:])\n",
    "        return next_token, logits\n",
    "\n",
    "    def sample_logits(self, logits: np.ndarray, sampling_method: str = \"greedy\", sampling_value: float = None, temperature: float = 1.0) -> np.ndarray:\n",
    "        if temperature == 0 or sampling_method == \"greedy\":\n",
    "            return np.argmax(logits, axis=-1).astype(np.int64)\n",
    "        elif sampling_method == \"top_k\" or sampling_method == \"top_p\":\n",
    "            assert sampling_value is not None\n",
    "            logits = logits.astype(np.float32)\n",
    "            logits /= temperature\n",
    "            probs = np.exp(logits) / np.sum(np.exp(logits))\n",
    "            sorted_probs = np.sort(probs)[:, ::-1]\n",
    "            sorted_indices = np.argsort(probs)[:, ::-1]\n",
    "\n",
    "            if sampling_method == \"top_k\":\n",
    "                index_of_interest = int(sampling_value)\n",
    "            elif sampling_method == \"top_p\":\n",
    "                p = sampling_value\n",
    "                cumulative_probs = np.cumsum(sorted_probs, axis=-1)\n",
    "                for index_of_interest, cumulative_prob in enumerate(cumulative_probs[0]):\n",
    "                    if cumulative_prob > p:\n",
    "                        break\n",
    "\n",
    "            probs_of_interest = sorted_probs[:, : index_of_interest + 1]\n",
    "            indices_of_interest = sorted_indices[:, : index_of_interest + 1]\n",
    "            probs_of_interest /= np.sum(probs_of_interest)\n",
    "            return np.array([np.random.choice(indices_of_interest[0], p=probs_of_interest[0])])\n",
    "        else:\n",
    "            raise Exception(f\"Unknown sampling method {sampling_method}\")\n",
    "\n",
    "    def predict(self, prompt, history=None, system_prompt=\"You are a helpful assistant.\", max_new_tokens=1024):\n",
    "        if history is None:\n",
    "            history = []\n",
    "        messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "        for (use_msg, bot_msg) in history:\n",
    "            messages.append({\"role\": \"user\", \"content\": use_msg})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": bot_msg})\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "        text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        if self.session_type == \"pytorch\":\n",
    "            input_ids = self.tokenizer([text], return_tensors=\"pt\")[\"input_ids\"].to(torch.long).reshape(1, -1)\n",
    "        else:\n",
    "            raise Exception(f\"unknown session_type {self.session_type}\")\n",
    "\n",
    "        input_ids = input_ids[:, -self.max_input_length:]\n",
    "        self.first = False\n",
    "        ids_list = []\n",
    "        input_length = input_ids.shape[1]\n",
    "        max_output_len = self.max_output_length - input_length\n",
    "        max_output_len = min(max_output_len, max_new_tokens)\n",
    "\n",
    "        for i in range(max_output_len):\n",
    "            logits = self.session.run(input_ids)\n",
    "            input_ids = self.sample_logits(logits[0][-1:])\n",
    "            input_ids = input_ids.reshape(1, -1)\n",
    "            ids_list.append(input_ids[0].item())\n",
    "            text_out = self.tokenizer.decode(ids_list)\n",
    "\n",
    "            # Early stop if EOS token is generated\n",
    "            if input_ids[0] == self.tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "        return text_out\n",
    "\n",
    "    def reset(self):\n",
    "        self.first = True\n",
    "        self.session.run_times = 0\n",
    "        self.session.reset()\n",
    "\n",
    "    def getState(self):\n",
    "        with self.lock:\n",
    "            return self.state.copy()\n",
    "\n",
    "# Hardcoded configuration (you can set values directly here)\n",
    "config = InferenceConfig(\n",
    "    hf_model_dir=\"/data/shaos/data/Qwen2.5-VL-3B-Instruct\",\n",
    "    om_model_path=\"path_to_om_model\",\n",
    "    onnx_model_path=\"path_to_onnx_model\",\n",
    "    cpu_thread=4,\n",
    "    session_type=\"pytorch\",\n",
    "    max_batch=1,\n",
    "    max_output_length=2048,\n",
    "    max_input_length=1024,\n",
    "    kv_cache_length=2048,\n",
    "    max_prefill_length=4,\n",
    "    dtype=\"float32\",\n",
    "    torch_dtype=\"float32\",\n",
    "    #tokenizer_dir=\"path_to_tokenizer\"\n",
    ")\n",
    "\n",
    "# Running the prediction\n",
    "inference = Inference(config)\n",
    "prompt = \"What is the capital of France?\"\n",
    "output = inference.predict(prompt)\n",
    "print(\"Generated Output:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0e34e8-09f2-489e-afa7-8931ea0e7c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c48bddba0f7e4f27a606aae33a9397a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] init success\n",
      "[INFO] NPU context set successfully\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "from transformers import AutoTokenizer\n",
    "from threading import Lock\n",
    "from utils.session import Session\n",
    "from config import InferenceConfig\n",
    "import torch\n",
    "import torch_npu\n",
    "\n",
    "class Inference:\n",
    "    def __init__(self, config: InferenceConfig) -> None:\n",
    "        # Initialize the necessary variables\n",
    "        self.max_input_length = config.max_input_length\n",
    "        self.max_output_length = config.max_output_length\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            config.tokenizer_dir, trust_remote_code=True\n",
    "        )\n",
    "        self.session = Session.fromConfig(config)\n",
    "        self.session_type = config.session_type\n",
    "        self.kv_cache_length = config.kv_cache_length\n",
    "        self.state: dict = {\"code\": 200, \"isEnd\": False, \"message\": \"\"}\n",
    "        self.reset()\n",
    "        self.lock = Lock()\n",
    "        self.first = True\n",
    "        print(\"[INFO] init success\")\n",
    "\n",
    "        # Set device to NPU\n",
    "        self.device = torch.device(\"npu\" if torch.npu.is_available() else \"cpu\")\n",
    "        print(\"[INFO] NPU context set successfully\")\n",
    "\n",
    "    def generate_cache(self, prompt: str):\n",
    "        if len(prompt) == 0:\n",
    "            return\n",
    "        self.first = False\n",
    "        input_ids = np.asarray(self.tokenizer.encode(prompt), dtype=np.int64).reshape(1, -1)\n",
    "        logits = self.session.run(input_ids)[0]\n",
    "        next_token = self.sample_logits(logits[0][-1:])\n",
    "        return next_token, logits\n",
    "\n",
    "    def sample_logits(self, logits: np.ndarray, sampling_method: str = \"greedy\", sampling_value: float = None, temperature: float = 1.0) -> np.ndarray:\n",
    "        if temperature == 0 or sampling_method == \"greedy\":\n",
    "            return np.argmax(logits, axis=-1).astype(np.int64)\n",
    "        elif sampling_method == \"top_k\" or sampling_method == \"top_p\":\n",
    "            assert sampling_value is not None\n",
    "            logits = logits.astype(np.float32)\n",
    "            logits /= temperature\n",
    "            probs = np.exp(logits) / np.sum(np.exp(logits))\n",
    "            sorted_probs = np.sort(probs)[:, ::-1]\n",
    "            sorted_indices = np.argsort(probs)[:, ::-1]\n",
    "\n",
    "            if sampling_method == \"top_k\":\n",
    "                index_of_interest = int(sampling_value)\n",
    "            elif sampling_method == \"top_p\":\n",
    "                p = sampling_value\n",
    "                cumulative_probs = np.cumsum(sorted_probs, axis=-1)\n",
    "                for index_of_interest, cumulative_prob in enumerate(cumulative_probs[0]):\n",
    "                    if cumulative_prob > p:\n",
    "                        break\n",
    "\n",
    "            probs_of_interest = sorted_probs[:, : index_of_interest + 1]\n",
    "            indices_of_interest = sorted_indices[:, : index_of_interest + 1]\n",
    "            probs_of_interest /= np.sum(probs_of_interest)\n",
    "            return np.array([np.random.choice(indices_of_interest[0], p=probs_of_interest[0])])\n",
    "        else:\n",
    "            raise Exception(f\"Unknown sampling method {sampling_method}\")\n",
    "\n",
    "    def predict(self, prompt, history=None, system_prompt=\"You are a helpful assistant.\", max_new_tokens=1024):\n",
    "        if history is None:\n",
    "            history = []\n",
    "        messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "        for (use_msg, bot_msg) in history:\n",
    "            messages.append({\"role\": \"user\", \"content\": use_msg})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": bot_msg})\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "        text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        if self.session_type == \"pytorch\":\n",
    "            input_ids = self.tokenizer([text], return_tensors=\"pt\")[\"input_ids\"].to(torch.long).reshape(1, -1)\n",
    "        else:\n",
    "            raise Exception(f\"unknown session_type {self.session_type}\")\n",
    "\n",
    "        input_ids = input_ids[:, -self.max_input_length:]\n",
    "        self.first = False\n",
    "        ids_list = []\n",
    "        input_length = input_ids.shape[1]\n",
    "        max_output_len = self.max_output_length - input_length\n",
    "        max_output_len = min(max_output_len, max_new_tokens)\n",
    "\n",
    "        for i in range(max_output_len):\n",
    "            logits = self.session.run(input_ids)\n",
    "            input_ids = self.sample_logits(logits[0][-1:])\n",
    "            input_ids = input_ids.reshape(1, -1)\n",
    "            ids_list.append(input_ids[0].item())\n",
    "            text_out = self.tokenizer.decode(ids_list)\n",
    "\n",
    "            # Early stop if EOS token is generated\n",
    "            if input_ids[0] == self.tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "        return text_out\n",
    "\n",
    "    def reset(self):\n",
    "        self.first = True\n",
    "        self.session.run_times = 0\n",
    "        self.session.reset()\n",
    "\n",
    "    def getState(self):\n",
    "        with self.lock:\n",
    "            return self.state.copy()\n",
    "\n",
    "# Hardcoded configuration (you can set values directly here)\n",
    "config = InferenceConfig(\n",
    "    hf_model_dir=\"/data/shaos/data/Qwen2.5-VL-3B-Instruct\",\n",
    "    om_model_path=\"path_to_om_model\",\n",
    "    onnx_model_path=\"path_to_onnx_model\",\n",
    "    cpu_thread=4,\n",
    "    session_type=\"pytorch\",\n",
    "    max_batch=1,\n",
    "    max_output_length=2048,\n",
    "    max_input_length=1024,\n",
    "    kv_cache_length=2048,\n",
    "    max_prefill_length=4,\n",
    "    dtype=\"float32\",\n",
    "    torch_dtype=\"float32\",\n",
    "    #tokenizer_dir=\"path_to_tokenizer\"\n",
    ")\n",
    "\n",
    "# Running the prediction\n",
    "inference = Inference(config)\n",
    "prompt = \"What is the capital of France?\"\n",
    "output = inference.predict(prompt)\n",
    "print(\"Generated Output:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1a2488-83c1-4e4e-a7a7-83a5e780b3ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
