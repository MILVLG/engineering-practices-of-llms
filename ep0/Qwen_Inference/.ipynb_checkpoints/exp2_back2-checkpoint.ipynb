{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae0e34e8-09f2-489e-afa7-8931ea0e7c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/shaos2/lib/python3.10/site-packages/torch_npu/__init__.py:234: UserWarning: On the interactive interface, the value of TASK_QUEUE_ENABLE is set to 0 by default.                      Do not set it to 1 to prevent some unknown errors\n",
      "  warnings.warn(\"On the interactive interface, the value of TASK_QUEUE_ENABLE is set to 0 by default. \\\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "240c219f44c448aba59e6ab35daa3278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] init success\n",
      "[INFO] NPU context set successfully\n",
      "Generated Output: The capital of France is Paris.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "from transformers import AutoTokenizer\n",
    "from threading import Lock\n",
    "from utils.session import Session\n",
    "from config import InferenceConfig\n",
    "import torch\n",
    "import torch_npu\n",
    "\n",
    "class Inference:\n",
    "    def __init__(self, config: InferenceConfig) -> None:\n",
    "        # Initialize the necessary variables\n",
    "        self.max_input_length = config.max_input_length\n",
    "        self.max_output_length = config.max_output_length\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            config.tokenizer_dir, trust_remote_code=True\n",
    "        )\n",
    "        self.session = Session.fromConfig(config)\n",
    "        self.session_type = config.session_type\n",
    "        self.kv_cache_length = config.kv_cache_length\n",
    "        self.state: dict = {\"code\": 200, \"isEnd\": False, \"message\": \"\"}\n",
    "        self.reset()\n",
    "        self.lock = Lock()\n",
    "        self.first = True\n",
    "        print(\"[INFO] init success\")\n",
    "\n",
    "        # Set device to NPU\n",
    "        # self.device = torch.device(\"npu\" if torch.npu.is_available() else \"cpu\")\n",
    "        self.device = torch.device(\"npu\")\n",
    "        print(\"[INFO] NPU context set successfully\")\n",
    "\n",
    "    def generate_cache(self, prompt: str):\n",
    "        if len(prompt) == 0:\n",
    "            return\n",
    "        self.first = False\n",
    "        input_ids = np.asarray(self.tokenizer.encode(prompt), dtype=np.int64).reshape(1, -1)\n",
    "        logits = self.session.run(input_ids)[0]\n",
    "        next_token = self.sample_logits(logits[0][-1:])\n",
    "        return next_token, logits\n",
    "\n",
    "    def sample_logits(self, logits: np.ndarray, sampling_method: str = \"greedy\", sampling_value: float = None, temperature: float = 1.0) -> np.ndarray:\n",
    "        if temperature == 0 or sampling_method == \"greedy\":\n",
    "            return np.argmax(logits, axis=-1).astype(np.int64)\n",
    "        elif sampling_method == \"top_k\" or sampling_method == \"top_p\":\n",
    "            assert sampling_value is not None\n",
    "            logits = logits.astype(np.float32)\n",
    "            logits /= temperature\n",
    "            probs = np.exp(logits) / np.sum(np.exp(logits))\n",
    "            sorted_probs = np.sort(probs)[:, ::-1]\n",
    "            sorted_indices = np.argsort(probs)[:, ::-1]\n",
    "\n",
    "            if sampling_method == \"top_k\":\n",
    "                index_of_interest = int(sampling_value)\n",
    "            elif sampling_method == \"top_p\":\n",
    "                p = sampling_value\n",
    "                cumulative_probs = np.cumsum(sorted_probs, axis=-1)\n",
    "                for index_of_interest, cumulative_prob in enumerate(cumulative_probs[0]):\n",
    "                    if cumulative_prob > p:\n",
    "                        break\n",
    "\n",
    "            probs_of_interest = sorted_probs[:, : index_of_interest + 1]\n",
    "            indices_of_interest = sorted_indices[:, : index_of_interest + 1]\n",
    "            probs_of_interest /= np.sum(probs_of_interest)\n",
    "            return np.array([np.random.choice(indices_of_interest[0], p=probs_of_interest[0])])\n",
    "        else:\n",
    "            raise Exception(f\"Unknown sampling method {sampling_method}\")\n",
    "\n",
    "    def predict(self, prompt, history=None, system_prompt=\"You are a helpful assistant.\", max_new_tokens=1024):\n",
    "        if history is None:\n",
    "            history = []\n",
    "        messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "        for (use_msg, bot_msg) in history:\n",
    "            messages.append({\"role\": \"user\", \"content\": use_msg})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": bot_msg})\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "        text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        if self.session_type == \"pytorch\":\n",
    "            input_ids = self.tokenizer([text], return_tensors=\"pt\")[\"input_ids\"].to(torch.long).reshape(1, -1)\n",
    "        else:\n",
    "            raise Exception(f\"unknown session_type {self.session_type}\")\n",
    "\n",
    "        input_ids = input_ids[:, -self.max_input_length:]\n",
    "        self.first = False\n",
    "        ids_list = []\n",
    "        input_length = input_ids.shape[1]\n",
    "        max_output_len = self.max_output_length - input_length\n",
    "        max_output_len = min(max_output_len, max_new_tokens)\n",
    "\n",
    "        for i in range(max_output_len):\n",
    "            logits = self.session.run(input_ids)\n",
    "            input_ids = self.sample_logits(logits[0][-1:])\n",
    "            input_ids = input_ids.reshape(1, -1)\n",
    "            ids_list.append(input_ids[0].item())\n",
    "            text_out = self.tokenizer.decode(ids_list)\n",
    "\n",
    "            # Early stop if EOS token is generated\n",
    "            if input_ids[0] == self.tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "        return text_out\n",
    "\n",
    "    def reset(self):\n",
    "        self.first = True\n",
    "        self.session.run_times = 0\n",
    "        self.session.reset()\n",
    "\n",
    "    def getState(self):\n",
    "        with self.lock:\n",
    "            return self.state.copy()\n",
    "\n",
    "# Hardcoded configuration (you can set values directly here)\n",
    "config = InferenceConfig(\n",
    "    hf_model_dir=\"/data/shaos/data/Qwen2.5-VL-3B-Instruct\",\n",
    "    om_model_path=\"path_to_om_model\",\n",
    "    onnx_model_path=\"path_to_onnx_model\",\n",
    "    cpu_thread=4,\n",
    "    session_type=\"pytorch\",\n",
    "    max_batch=1,\n",
    "    max_output_length=2048,\n",
    "    max_input_length=1024,\n",
    "    kv_cache_length=2048,\n",
    "    max_prefill_length=4,\n",
    "    dtype=\"float32\",\n",
    "    torch_dtype=\"float32\",\n",
    "    #tokenizer_dir=\"path_to_tokenizer\"\n",
    ")\n",
    "\n",
    "# Running the prediction\n",
    "inference = Inference(config)\n",
    "prompt = \"What is the capital of France?\"\n",
    "output = inference.predict(prompt)\n",
    "print(\"Generated Output:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141560a9-79b2-4813-b9c2-839ce2c97d70",
   "metadata": {},
   "source": [
    "# 1. 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ba1a2488-83c1-4e4e-a7a7-83a5e780b3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "from transformers import AutoTokenizer\n",
    "from threading import Lock\n",
    "from utils.session import Session\n",
    "from config import InferenceConfig\n",
    "import torch\n",
    "import torch_npu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32760b65-8f33-45b5-ba1d-4f2b110f1d4c",
   "metadata": {},
   "source": [
    "numpy：用于处理数组和进行数学运算的库。\n",
    "\n",
    "os：提供与操作系统交互的功能，如文件处理。\n",
    "\n",
    "time 和 gc：用于时间相关的操作和垃圾回收。\n",
    "\n",
    "AutoTokenizer：来自 Hugging Face，用于加载预训练模型的分词器，将文本转化为模型可以理解的token。\n",
    "\n",
    "Lock：来自threading模块，用于线程同步。\n",
    "\n",
    "Session：自定义类，可能用于管理模型的会话。\n",
    "\n",
    "InferenceConfig：自定义配置类，包含模型的超参数设置。\n",
    "\n",
    "torch 和 torch_npu：PyTorch库，torch_npu用于处理NPU（神经处理单元）相关操作。\n",
    "\n",
    "# 2. Inference 类\n",
    "\n",
    "Inference 类是脚本的核心，负责与模型交互，包括加载模型、处理输入、生成预测以及管理会话状态。\n",
    "\n",
    "## 初始化 (__init__ 方法)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "902accec-b121-453d-bdbb-31c83d4bea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inference:\n",
    "    def __init__(self, config: InferenceConfig) -> None:\n",
    "        # Initialize the necessary variables\n",
    "        self.max_input_length = config.max_input_length\n",
    "        self.max_output_length = config.max_output_length\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            config.tokenizer_dir, trust_remote_code=True\n",
    "        )\n",
    "        self.session = Session.fromConfig(config)\n",
    "        self.session_type = config.session_type\n",
    "        self.kv_cache_length = config.kv_cache_length\n",
    "        self.state: dict = {\"code\": 200, \"isEnd\": False, \"message\": \"\"}\n",
    "        self.reset()\n",
    "        self.lock = Lock()\n",
    "        self.first = True\n",
    "        print(\"[INFO] init success\")\n",
    "\n",
    "        # Set device to NPU\n",
    "        # self.device = torch.device(\"npu\" if torch.npu.is_available() else \"cpu\")\n",
    "        self.device = torch.device(\"npu\")\n",
    "        print(\"[INFO] NPU context set successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fee297d-c7c1-41fd-9e38-b09b654f6121",
   "metadata": {},
   "source": [
    "self.max_input_length 和 self.max_output_length：从配置中读取最大输入和输出长度，用于控制模型处理的文本长度。\n",
    "\n",
    "AutoTokenizer：加载一个预训练的分词器，将输入的文本转换成模型可以理解的token。\n",
    "\n",
    "Session：自定义的类，用于管理和配置模型会话。\n",
    "\n",
    "self.device：将计算设备设置为 NPU（如果可用），否则默认为CPU。\n",
    "\n",
    "## 缓存生成 (generate_cache 方法)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8eea5056-a44e-414e-9e70-ec6908c09cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def generate_cache(self, prompt: str):\n",
    "        if len(prompt) == 0:\n",
    "            return\n",
    "        self.first = False\n",
    "        input_ids = np.asarray(self.tokenizer.encode(prompt), dtype=np.int64).reshape(1, -1)\n",
    "        logits = self.session.run(input_ids)[0]\n",
    "        next_token = self.sample_logits(logits[0][-1:])\n",
    "        return next_token, logits\n",
    "\n",
    "    Inference.generate_cache = generate_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0797f9a2-38e7-42fa-a36b-bba28b896624",
   "metadata": {},
   "source": [
    "这个方法接收一个提示文本（prompt），将其转换为token（input_ids），然后通过模型生成预测的下一个token及其logits（原始输出）。\n",
    "\n",
    "## 采样下一个token (sample_logits 方法)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "314b4939-6927-4188-9cc9-8092630be348",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def sample_logits(self, logits: np.ndarray, sampling_method: str = \"greedy\", sampling_value: float = None, temperature: float = 1.0) -> np.ndarray:\n",
    "        if temperature == 0 or sampling_method == \"greedy\":\n",
    "            return np.argmax(logits, axis=-1).astype(np.int64)\n",
    "        elif sampling_method == \"top_k\" or sampling_method == \"top_p\":\n",
    "            assert sampling_value is not None\n",
    "            logits = logits.astype(np.float32)\n",
    "            logits /= temperature\n",
    "            probs = np.exp(logits) / np.sum(np.exp(logits))\n",
    "            sorted_probs = np.sort(probs)[:, ::-1]\n",
    "            sorted_indices = np.argsort(probs)[:, ::-1]\n",
    "\n",
    "            if sampling_method == \"top_k\":\n",
    "                index_of_interest = int(sampling_value)\n",
    "            elif sampling_method == \"top_p\":\n",
    "                p = sampling_value\n",
    "                cumulative_probs = np.cumsum(sorted_probs, axis=-1)\n",
    "                for index_of_interest, cumulative_prob in enumerate(cumulative_probs[0]):\n",
    "                    if cumulative_prob > p:\n",
    "                        break\n",
    "\n",
    "            probs_of_interest = sorted_probs[:, : index_of_interest + 1]\n",
    "            indices_of_interest = sorted_indices[:, : index_of_interest + 1]\n",
    "            probs_of_interest /= np.sum(probs_of_interest)\n",
    "            return np.array([np.random.choice(indices_of_interest[0], p=probs_of_interest[0])])\n",
    "        else:\n",
    "            raise Exception(f\"Unknown sampling method {sampling_method}\")\n",
    "\n",
    "    Inference.sample_logits = sample_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "812daa55-667f-4e90-9ae7-0ed83bbe2b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def reset(self):\n",
    "        self.first = True\n",
    "        self.session.run_times = 0\n",
    "        self.session.reset()\n",
    "    \n",
    "    # 把这个函数挂到 Inference 类上，作为方法\n",
    "    Inference.reset = reset\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "75b7b7c8-baa2-46ac-968e-8f1306f9d03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def getState(self):\n",
    "        with self.lock:\n",
    "            return self.state.copy()\n",
    "\n",
    "    Inference.getState = getState"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b21296-6718-4862-8773-d4c1ab690653",
   "metadata": {},
   "source": [
    "sampling_method：决定如何选择下一个token（例如，贪婪算法（greedy）、Top-k采样、Top-p采样）。\n",
    "\n",
    "temperature：控制生成文本的随机性。温度越高，生成的文本越多样化。\n",
    "\n",
    "## 预测 (predict 方法)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cfadce71-be1f-4382-9f7d-c7fd73fd6773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'generate_cache', 'getState', 'predict', 'reset', 'sample_logits']\n"
     ]
    }
   ],
   "source": [
    "    def predict(self, prompt, history=None, system_prompt=\"You are a helpful assistant.\", max_new_tokens=1024):\n",
    "        if history is None:\n",
    "            history = []\n",
    "        messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "        for (use_msg, bot_msg) in history:\n",
    "            messages.append({\"role\": \"user\", \"content\": use_msg})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": bot_msg})\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "        text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        if self.session_type == \"pytorch\":\n",
    "            input_ids = self.tokenizer([text], return_tensors=\"pt\")[\"input_ids\"].to(torch.long).reshape(1, -1)\n",
    "        else:\n",
    "            raise Exception(f\"unknown session_type {self.session_type}\")\n",
    "\n",
    "        input_ids = input_ids[:, -self.max_input_length:]\n",
    "        self.first = False\n",
    "        ids_list = []\n",
    "        input_length = input_ids.shape[1]\n",
    "        max_output_len = self.max_output_length - input_length\n",
    "        max_output_len = min(max_output_len, max_new_tokens)\n",
    "\n",
    "        for i in range(max_output_len):\n",
    "            logits = self.session.run(input_ids)\n",
    "            input_ids = self.sample_logits(logits[0][-1:])\n",
    "            input_ids = input_ids.reshape(1, -1)\n",
    "            ids_list.append(input_ids[0].item())\n",
    "            text_out = self.tokenizer.decode(ids_list)\n",
    "\n",
    "            # Early stop if EOS token is generated\n",
    "            if input_ids[0] == self.tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "        return text_out\n",
    "    Inference.predict = predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2b4113-d578-4806-a293-9bc95c46978f",
   "metadata": {},
   "source": [
    "该方法通过递归生成token来实现文本生成。它会通过会话获取模型的输出，并根据指定的最大输出长度生成响应。\n",
    "\n",
    "## 3. 配置设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "431af149-db7e-45c6-82a6-f054f101db90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hardcoded configuration (you can set values directly here)\n",
    "config = InferenceConfig(\n",
    "    hf_model_dir=\"/data/shaos/data/Qwen2.5-VL-3B-Instruct\",\n",
    "    om_model_path=\"path_to_om_model\",\n",
    "    onnx_model_path=\"path_to_onnx_model\",\n",
    "    cpu_thread=4,\n",
    "    session_type=\"pytorch\",\n",
    "    max_batch=1,\n",
    "    max_output_length=2048,\n",
    "    max_input_length=1024,\n",
    "    kv_cache_length=2048,\n",
    "    max_prefill_length=4,\n",
    "    dtype=\"float32\",\n",
    "    torch_dtype=\"float32\",\n",
    "    #tokenizer_dir=\"path_to_tokenizer\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208b6454-b9a9-4cab-9e16-f2cbe9c08559",
   "metadata": {},
   "source": [
    "InferenceConfig：这个配置类包含了模型加载、设备设置、最大输入和输出长度等各种超参数和路径。\n",
    "\n",
    "## 4. 执行预测\n",
    "\n",
    "创建一个 Inference 对象，传入配置，并使用 predict 方法对给定的提示（\"What is the capital of France?\"）进行推理，生成并打印输出结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e08f380e-ed63-4585-8734-41d331e6d8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf93e81fb5c4d798fb79e16c9e9dd0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] init success\n",
      "[INFO] NPU context set successfully\n",
      "Generated Output: The capital of France is Paris.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Running the prediction\n",
    "inference = Inference(config)\n",
    "prompt = \"What is the capital of France?\"\n",
    "output = inference.predict(prompt)\n",
    "print(\"Generated Output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d4d0e4-19fc-4f42-b334-8c58035552d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "关键概念和解释：\n",
    "\n",
    "模型推理：指的是使用预训练的模型生成预测结果。在本脚本中，推理过程是通过传入文本提示并生成下一步最可能的token来实现的。\n",
    "\n",
    "采样方法：脚本支持多种方法来选择下一个token（例如 贪婪（Greedy）、Top-k、Top-p）。贪婪方法简单地选择概率最高的token，而Top-k和Top-p则引入了一定的随机性，使生成的文本更加自然和多样。\n",
    "\n",
    "会话管理：Session 类用于处理与模型的交互，它管理输入、输出、状态等。\n",
    "\n",
    "NPU设备：如果支持NPU（神经处理单元），脚本会尝试使用NPU进行加速计算。如果没有NPU，它会回退到CPU或GPU。\n",
    "\n",
    "状态重置：每次预测后，都会重置模型的状态，确保不会受到先前预测的影响。\n",
    "\n",
    "建议的练习：\n",
    "\n",
    "修改采样方法：尝试使用不同的采样方法（贪婪、Top-k、Top-p），观察它们如何影响生成文本的多样性和准确性。\n",
    "\n",
    "调整输入输出长度：修改 max_input_length 和 max_output_length，看看这些参数如何影响模型的性能，尤其是在处理较长文本时。\n",
    "\n",
    "优化推理速度：尝试批处理并设置不同的设备（例如使用GPU或CPU），以改善推理速度，尤其是对于大型模型。\n",
    "\n",
    "使用不同的提示：更改 predict 方法中的提示，观察模型对不同类型的查询（事实性问题、创意问题等）的响应。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
