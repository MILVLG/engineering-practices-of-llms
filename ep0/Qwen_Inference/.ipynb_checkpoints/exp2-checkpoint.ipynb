{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "141560a9-79b2-4813-b9c2-839ce2c97d70",
   "metadata": {},
   "source": [
    "# 1. 导入库\n",
    "在这部分，我们将引入所需的Python库，这些库将帮助我们进行数据处理、模型加载、推理等操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba1a2488-83c1-4e4e-a7a7-83a5e780b3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "from transformers import AutoTokenizer\n",
    "from threading import Lock\n",
    "from utils.session import Session\n",
    "from config import InferenceConfig\n",
    "import torch\n",
    "import torch_npu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32760b65-8f33-45b5-ba1d-4f2b110f1d4c",
   "metadata": {},
   "source": [
    "numpy：用于处理数组和进行数学运算的库。\n",
    "\n",
    "os：提供与操作系统交互的功能，如文件处理。\n",
    "\n",
    "time 和 gc：用于时间相关的操作和垃圾回收。\n",
    "\n",
    "AutoTokenizer：来自 Hugging Face，用于加载预训练模型的分词器，将文本转化为模型可以理解的token。\n",
    "\n",
    "Lock：来自threading模块，用于线程同步。\n",
    "\n",
    "Session：自定义类，可能用于管理模型的会话。\n",
    "\n",
    "InferenceConfig：自定义配置类，包含模型的超参数设置。\n",
    "\n",
    "torch 和 torch_npu：PyTorch库，torch_npu用于处理NPU（神经处理单元）相关操作。\n",
    "\n",
    "# 2. Inference 类\n",
    "\n",
    "Inference 类是脚本的核心，负责与模型交互，包括加载模型、处理输入、生成预测以及管理会话状态。\n",
    "\n",
    "## 初始化 (__init__ 方法)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "902accec-b121-453d-bdbb-31c83d4bea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inference:\n",
    "    def __init__(self, config: InferenceConfig) -> None:\n",
    "        # Initialize the necessary variables\n",
    "        self.max_input_length = config.max_input_length\n",
    "        self.max_output_length = config.max_output_length\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            config.tokenizer_dir, trust_remote_code=True\n",
    "        )\n",
    "        self.session = Session.fromConfig(config)\n",
    "        self.session_type = config.session_type\n",
    "        self.kv_cache_length = config.kv_cache_length\n",
    "        self.state: dict = {\"code\": 200, \"isEnd\": False, \"message\": \"\"}\n",
    "        self.reset()\n",
    "        self.lock = Lock()\n",
    "        self.first = True\n",
    "        print(\"[INFO] init success\")\n",
    "\n",
    "        # Set device to NPU\n",
    "        # self.device = torch.device(\"npu\" if torch.npu.is_available() else \"cpu\")\n",
    "        # self.device = torch.device(\"npu\")\n",
    "        print(\"[INFO] NPU context set successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fee297d-c7c1-41fd-9e38-b09b654f6121",
   "metadata": {},
   "source": [
    "self.max_input_length 和 self.max_output_length：从配置中读取最大输入和输出长度，用于控制模型处理的文本长度。\n",
    "\n",
    "AutoTokenizer：加载一个预训练的分词器，将输入的文本转换成模型可以理解的token。\n",
    "\n",
    "Session：自定义的类，用于管理和配置模型会话。\n",
    "\n",
    "self.device：将计算设备设置为 NPU（如果可用），否则默认为CPU。\n",
    "\n",
    "## 3.缓存生成 (generate_cache 方法)\n",
    "该方法接收输入文本，将其转换为token，并通过模型生成预测结果。 \n",
    "它也会计算下一个token并返回其logits（原始模型输出）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eea5056-a44e-414e-9e70-ec6908c09cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def generate_cache(self, prompt: str):\n",
    "        if len(prompt) == 0:\n",
    "            return\n",
    "        self.first = False\n",
    "        input_ids = np.asarray(self.tokenizer.encode(prompt), dtype=np.int64).reshape(1, -1)\n",
    "        logits = self.session.run(input_ids)[0]\n",
    "        next_token = self.sample_logits(logits[0][-1:])\n",
    "        return next_token, logits\n",
    "\n",
    "    Inference.generate_cache = generate_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0797f9a2-38e7-42fa-a36b-bba28b896624",
   "metadata": {},
   "source": [
    "## 4. 采样下一个token (sample_logits 方法)\n",
    "该方法使用不同的采样方法（如贪婪、Top-k、Top-p）来选择下一个最可能的token。\n",
    "它允许通过设置不同的参数来调整生成文本的多样性和随机性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "314b4939-6927-4188-9cc9-8092630be348",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def sample_logits(self, logits: np.ndarray, sampling_method: str = \"greedy\", sampling_value: float = None, temperature: float = 1.0) -> np.ndarray:\n",
    "        if temperature == 0 or sampling_method == \"greedy\":\n",
    "            return np.argmax(logits, axis=-1).astype(np.int64)\n",
    "        elif sampling_method == \"top_k\" or sampling_method == \"top_p\":\n",
    "            assert sampling_value is not None\n",
    "            logits = logits.astype(np.float32)\n",
    "            logits /= temperature\n",
    "            probs = np.exp(logits) / np.sum(np.exp(logits))\n",
    "            sorted_probs = np.sort(probs)[:, ::-1]\n",
    "            sorted_indices = np.argsort(probs)[:, ::-1]\n",
    "\n",
    "            if sampling_method == \"top_k\":\n",
    "                index_of_interest = int(sampling_value)\n",
    "            elif sampling_method == \"top_p\":\n",
    "                p = sampling_value\n",
    "                cumulative_probs = np.cumsum(sorted_probs, axis=-1)\n",
    "                for index_of_interest, cumulative_prob in enumerate(cumulative_probs[0]):\n",
    "                    if cumulative_prob > p:\n",
    "                        break\n",
    "\n",
    "            probs_of_interest = sorted_probs[:, : index_of_interest + 1]\n",
    "            indices_of_interest = sorted_indices[:, : index_of_interest + 1]\n",
    "            probs_of_interest /= np.sum(probs_of_interest)\n",
    "            return np.array([np.random.choice(indices_of_interest[0], p=probs_of_interest[0])])\n",
    "        else:\n",
    "            raise Exception(f\"Unknown sampling method {sampling_method}\")\n",
    "\n",
    "    Inference.sample_logits = sample_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27612464-90aa-4934-901f-1c272227fc67",
   "metadata": {},
   "source": [
    "# 5. 状态重置 (reset 方法)\n",
    "这个方法会在每次推理后重置模型的状态，确保新的预测不会受到旧的预测结果的影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "812daa55-667f-4e90-9ae7-0ed83bbe2b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def reset(self):\n",
    "        self.first = True\n",
    "        self.session.run_times = 0\n",
    "        self.session.reset()\n",
    "    \n",
    "    # 把这个函数挂到 Inference 类上，作为方法\n",
    "    Inference.reset = reset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cf840e-bda6-49c5-bbda-7589b45c37e9",
   "metadata": {},
   "source": [
    "# 6. 获取状态 (getState 方法)\n",
    "这个方法用于获取当前会话的状态，返回一个状态的副本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75b7b7c8-baa2-46ac-968e-8f1306f9d03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def getState(self):\n",
    "        with self.lock:\n",
    "            return self.state.copy()\n",
    "\n",
    "    Inference.getState = getState"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3324f4f0-df2e-4643-8ac6-c6119e863d76",
   "metadata": {},
   "source": [
    "# 7. 关闭会话 (close 方法)\n",
    "该方法会在结束时清理资源，删除缓存并执行垃圾回收，确保没有未清理的资源占用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc7c7222-435d-43fd-ae41-520cf7a11cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def close(self):\n",
    "        if hasattr(self, \"session\"):\n",
    "            if hasattr(self.session, \"kv_cache\"):\n",
    "                del self.session.kv_cache\n",
    "            del self.session.model\n",
    "        gc.collect()\n",
    "        torch_npu.npu.empty_cache()\n",
    "        torch_npu.npu.synchronize()\n",
    "\n",
    "    Inference.close = close"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b21296-6718-4862-8773-d4c1ab690653",
   "metadata": {},
   "source": [
    "sampling_method：决定如何选择下一个token（例如，贪婪算法（greedy）、Top-k采样、Top-p采样）。\n",
    "\n",
    "temperature：控制生成文本的随机性。温度越高，生成的文本越多样化。\n",
    "\n",
    "# 8. 预测 (predict 方法)\n",
    "这是模型推理的主方法，通过递归生成token来生成最终的文本输出。\n",
    "它支持多种采样方法，可以生成对不同输入的预测结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfadce71-be1f-4382-9f7d-c7fd73fd6773",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def predict(self, prompt, history=None, system_prompt=\"You are a helpful assistant.\", max_new_tokens=1024, sampling_method=\"greedy\", sampling_value=None, temperature=1.0):\n",
    "        \"\"\"\n",
    "        添加了 sampling_method 参数，允许选择不同的采样方法（greedy, top_k, top_p）\n",
    "        \"\"\"\n",
    "        if history is None:\n",
    "            history = []\n",
    "        \n",
    "        # 处理消息\n",
    "        messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "        for (use_msg, bot_msg) in history:\n",
    "            messages.append({\"role\": \"user\", \"content\": use_msg})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": bot_msg})\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "        # 将消息转换为token\n",
    "        text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        input_ids = self.tokenizer([text], return_tensors=\"pt\")[\"input_ids\"].to(torch.long).reshape(1, -1)\n",
    "\n",
    "        # 限制输入的最大长度\n",
    "        input_ids = input_ids[:, -self.max_input_length:]\n",
    "        ids_list = []\n",
    "        input_length = input_ids.shape[1]\n",
    "        max_output_len = self.max_output_length - input_length\n",
    "        max_output_len = min(max_output_len, max_new_tokens)\n",
    "\n",
    "        for i in range(max_output_len):\n",
    "            # 运行模型得到logits\n",
    "            logits = self.session.run(input_ids)\n",
    "            \n",
    "            # 使用传入的采样方法（sampling_method）\n",
    "            input_ids = self.sample_logits(logits[0][-1:], sampling_method=sampling_method, sampling_value=sampling_value, temperature=temperature)\n",
    "            input_ids = input_ids.reshape(1, -1)\n",
    "            ids_list.append(input_ids[0].item())\n",
    "            text_out = self.tokenizer.decode(ids_list)\n",
    "\n",
    "            # 如果遇到EOS token则提前停止\n",
    "            if input_ids[0] == self.tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "        return text_out\n",
    "\n",
    "    # 确保这一行没有额外的缩进\n",
    "    Inference.predict = predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2b4113-d578-4806-a293-9bc95c46978f",
   "metadata": {},
   "source": [
    "该方法通过递归生成token来实现文本生成。它会通过会话获取模型的输出，并根据指定的最大输出长度生成响应。\n",
    "\n",
    "## 9. 配置设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "431af149-db7e-45c6-82a6-f054f101db90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hardcoded configuration (you can set values directly here)\n",
    "config = InferenceConfig(\n",
    "    hf_model_dir=\"/data/shaos/data/Qwen2.5-VL-3B-Instruct\",\n",
    "    om_model_path=\"path_to_om_model\",\n",
    "    onnx_model_path=\"path_to_onnx_model\",\n",
    "    cpu_thread=4,\n",
    "    session_type=\"pytorch\",\n",
    "    max_batch=1,\n",
    "    max_output_length=2048,\n",
    "    max_input_length=1024,\n",
    "    kv_cache_length=2048,\n",
    "    max_prefill_length=4,\n",
    "    dtype=\"float32\",\n",
    "    torch_dtype=\"float32\",\n",
    "    device_str=\"npu\"\n",
    "    #tokenizer_dir=\"path_to_tokenizer\"\n",
    ")\n",
    "\n",
    "config.session_type = \"pytorch\"\n",
    "config.kvcache_method = \"fixsize\"   # 保持和导出 config 一致"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208b6454-b9a9-4cab-9e16-f2cbe9c08559",
   "metadata": {},
   "source": [
    "# InferenceConfig：这个配置类包含了模型加载、设备设置、最大输入和输出长度等各种超参数和路径。\n",
    "\n",
    "## 10. 执行预测\n",
    "\n",
    "创建一个 Inference 对象，传入配置，并使用 predict 方法对给定的提示（\"What is the capital of France?\"）进行推理，生成并打印输出结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e08f380e-ed63-4585-8734-41d331e6d8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] PyTorchSession device = npu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba3b0a946fb14e55adae9c303c2e32da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] init success\n",
      "[INFO] NPU context set successfully\n",
      "Generated Output: The capital of France is Paris.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Running the prediction\n",
    "inference = Inference(config)\n",
    "prompt = \"What is the capital of France?\"\n",
    "output = inference.predict(prompt)\n",
    "print(\"Generated Output:\", output)\n",
    "\n",
    "inference.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d4d0e4-19fc-4f42-b334-8c58035552d8",
   "metadata": {},
   "source": [
    "关键概念和解释：\n",
    "\n",
    "模型推理：指的是使用预训练的模型生成预测结果。在本脚本中，推理过程是通过传入文本提示并生成下一步最可能的token来实现的。\n",
    "\n",
    "采样方法：脚本支持多种方法来选择下一个token（例如 贪婪（Greedy）、Top-k、Top-p）。贪婪方法简单地选择概率最高的token，而Top-k和Top-p则引入了一定的随机性，使生成的文本更加自然和多样。\n",
    "\n",
    "会话管理：Session 类用于处理与模型的交互，它管理输入、输出、状态等。\n",
    "\n",
    "NPU设备：如果支持NPU（神经处理单元），脚本会尝试使用NPU进行加速计算。如果没有NPU，它会回退到CPU或GPU。\n",
    "\n",
    "状态重置：每次预测后，都会重置模型的状态，确保不会受到先前预测的影响。\n",
    "\n",
    "建议的练习：\n",
    "\n",
    "修改采样方法：尝试使用不同的采样方法（贪婪、Top-k、Top-p），观察它们如何影响生成文本的多样性和准确性。\n",
    "\n",
    "调整输入输出长度：修改 max_input_length 和 max_output_length，看看这些参数如何影响模型的性能，尤其是在处理较长文本时。\n",
    "\n",
    "优化推理速度：尝试批处理并设置不同的设备（例如使用GPU或CPU），以改善推理速度，尤其是对于大型模型。\n",
    "\n",
    "使用不同的提示：更改 predict 方法中的提示，观察模型对不同类型的查询（事实性问题、创意问题等）的响应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3bbc24b-659c-4b94-b2a8-3cc7d98a5053",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] PyTorchSession device = npu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ddfcf8e5c04b13a5b92e1cecc46a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] init success\n",
      "[INFO] NPU context set successfully\n",
      "Greedy Sampling Output:\n",
      "The capital of France is Paris.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the capital of France?\"\n",
    "inference = Inference(config)\n",
    "# 使用贪婪采样\n",
    "output_greedy = inference.predict(prompt, sampling_method=\"greedy\")\n",
    "\n",
    "print(\"Greedy Sampling Output:\")\n",
    "print(output_greedy)\n",
    "\n",
    "inference.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e08457b-2e35-49ec-8517-b9f301516edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] PyTorchSession device = npu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8e13115b598452ea4e57f9b69b8ffd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] init success\n",
      "[INFO] NPU context set successfully\n",
      "Top-K Sampling Output:\n",
      "Artificial intelligence (AI) has the potential to revolutionize many fields including healthcare, finance, education, and more. Here are some of the benefits of AI:\n",
      "\n",
      "1. Improved accuracy and efficiency: AI systems can analyze large amounts of data and provide accurate insights and predictions that humans might miss. This can lead to increased efficiency and productivity.\n",
      "\n",
      "2. Increased accessibility: AI can be used to create applications and tools that make complex processes easier to understand and use, making them more accessible to individuals with varying levels of expertise.\n",
      "\n",
      "3. Improved decision making: AI can help individuals and organizations make data-driven decisions by providing them with the most accurate and relevant information.\n",
      "\n",
      "4. Enhanced personalization: AI can be used to create personalized experiences for individuals, such as in the case of personalized healthcare or tailored marketing messages.\n",
      "\n",
      "5. Increased security: AI can help organizations detect and prevent security breaches by analyzing and identifying patterns in data that may indicate a threat.\n",
      "\n",
      "6. Increased creativity: AI can be used to create new and innovative products, services, and ideas by analyzing data and identifying new opportunities for improvement.\n",
      "\n",
      "Overall, AI has the potential to greatly improve many aspects of our lives and industries, leading to increased productivity, efficiency, and innovation.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What are the benefits of artificial intelligence?\"\n",
    "inference = Inference(config)\n",
    "# 使用Top-k采样，选择概率前5的token\n",
    "output_top_k = inference.predict(prompt, sampling_method=\"top_k\", sampling_value=5)\n",
    "\n",
    "print(\"Top-K Sampling Output:\")\n",
    "print(output_top_k)\n",
    "\n",
    "inference.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fe6b7e8-6b45-46ee-913d-d0600c1b074f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] PyTorchSession device = npu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a6705131b004f529e62eb779d2c848d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] init success\n",
      "[INFO] NPU context set successfully\n",
      "Top-P Sampling Output:\n",
      "Quantum computing is a type of computing that uses the principles of quantum mechanics to perform calculations and solve problems. Unlike classical computing, which uses bits to represent and process information, quantum computing uses quantum bits, or qubits, which can represent multiple states simultaneously.\n",
      "In classical computing, a bit can be either 0 or 1. In quantum computing, a qubit can be a 0, a 1, or a combination of both. This ability to represent multiple states simultaneously allows quantum computers to process vast amounts of information and solve problems that are infeasible for classical computers to solve.\n",
      "Quantum computers use a complex system of quantum bits called a quantum computer, which can perform multiple calculations at the same time. This ability to perform multiple calculations at the same time is known as parallelism and allows quantum computers to solve problems much faster than classical computers.\n",
      "However, quantum computing is still in its infancy, and there are several challenges to overcome before it can be used for practical applications. One of the biggest challenges is the fragility of quantum systems, which can be easily disturbed by their environment, making it difficult to maintain the qubits in a stable state.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Can you explain quantum computing?\"\n",
    "inference = Inference(config)\n",
    "# 使用Top-p采样，选择累积概率不超过0.9的token\n",
    "output_top_p = inference.predict(prompt, sampling_method=\"top_p\", sampling_value=0.9)\n",
    "\n",
    "print(\"Top-P Sampling Output:\")\n",
    "print(output_top_p)\n",
    "\n",
    "inference.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "183cf39a-fc79-4804-a4d4-8aaa033e28fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] PyTorchSession device = npu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e42a3cfa4db4c95995e38b6e4ebe4d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] init success\n",
      "[INFO] NPU context set successfully\n",
      "Short Input/Output Example:\n",
      "2 + 2 is equal to 4.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is 2 + 2?\"\n",
    "inference = Inference(config)\n",
    "# 设置较短的输入和输出长度\n",
    "output_short = inference.predict(prompt, max_new_tokens=64)\n",
    "\n",
    "print(\"Short Input/Output Example:\")\n",
    "print(output_short)\n",
    "\n",
    "inference.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9673ea1b-8d4a-4dbf-978b-88d542b7e121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] PyTorchSession device = npu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98129e64849247adaf43dd780bcbbf9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] init success\n",
      "[INFO] NPU context set successfully\n",
      "Long Input/Output Example:\n",
      "Quantum entanglement is a phenomenon in which two or more particles become interconnected and their quantum states become correlated, even when they are separated by large distances. This means that the state of one particle can be instantly determined by the state of the other, regardless of the distance between them. This phenomenon was first observed in\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Can you explain how quantum entanglement works and its potential applications in future technologies?\"\n",
    "inference = Inference(config)\n",
    "# 设置较长的输入和输出长度\n",
    "output_long = inference.predict(prompt, max_new_tokens=64)\n",
    "\n",
    "print(\"Long Input/Output Example:\")\n",
    "print(output_long)\n",
    "\n",
    "inference.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b743444-1621-423d-a92a-7aa5aac3f932",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] PyTorchSession device = npu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31960ae4d07a45ceaca0557e828607d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] init success\n",
      "[INFO] NPU context set successfully\n",
      "Extreme Length Input/Output Example:\n",
      "Machine learning algorithms are a type of artificial intelligence that allows computers to learn from data and make predictions or decisions without being explicitly programmed. There are three main types of machine learning algorithms: supervised learning, unsupervised learning, and reinforcement learning.\n",
      "\n",
      "Supervised learning is a type of machine learning where the algorithm is trained on a labeled dataset, meaning that the input data is accompanied by the correct output. The algorithm learns to map the input data to the correct output by finding a function that best fits the data. Supervised learning is used for tasks such as classification and regression, where the algorithm is trained to predict a specific output based on the input\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Can you give me a detailed explanation of machine learning algorithms, including supervised learning, unsupervised learning, and reinforcement learning, and provide examples of each?\"\n",
    "inference = Inference(config)\n",
    "# 设置超长的输入和输出长度\n",
    "output_extreme = inference.predict(prompt, max_new_tokens=128)\n",
    "\n",
    "print(\"Extreme Length Input/Output Example:\")\n",
    "print(output_extreme)\n",
    "\n",
    "inference.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb6afee-97ae-4777-9dac-f11ad6ecfaac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
